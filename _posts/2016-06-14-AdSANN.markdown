---
layout: post
title:  "The AdS/ANN Correspondence"
date:   2016-06-12 17:02:02 +0100
categories:
comments: true
---

This post is about Artificial Neural Networks (or just neural networks for short), which have completely and utterly distracted me for the last 2 months or so, and my attempts to understand them through the lens of physics. My hope is that this post will be enjoyable for the physics and machine learning communities alike.

First, let me comment on the title. I would argue that one of the most profound scientific discoveries in my lifetime has been the [AdS/CFT Correspondence](https://en.wikipedia.org/wiki/AdS/CFT_correspondence) which is a duality, or equivalence between two very different types of theories. Here AdS stands for Anti-de Sitter space, the maximally symmetric negatively curved spacetime. This sounds like a mouthful but the idea is very simple--it’s basically the spacetime analog of a hyperbolic space in the same way that Minkowski space is the spacetime analog of flat Euclidean space \\( \mathbb{R}^4 \\). An example of a negatively curved space is depicted in this M.C. Escher painting

<p align="center">
    <img src="/assets/AdSANN/escher.jpg"/>
</p>

The other ingredient in the AdS/CFT correspondence is CFT, which stands for conformal field theory. Quantum field theories basically describe all of known physics which the exception of gravity, which has proven very hard to quantize, and conformal field theories have a special symmetry which is a generalization of scale invariance. AdS/CFT is a *duality* between these two very different types of theories--even though they look completely unrelated they are apparently describing the same underlying physics!

Ok, so I thought this article was about neural networks, not AdS/CFT? A major theme in theoretical physics in the past two decade or so has been dualities, of which AdS/CFT is one example, where two very different systems are shown to be equivalent [^1]. These dualities are such that often a hard problem on one side of the duality becomes easy on the other, and have therefore proven immensely useful. The number of dualities is growing at a such a rapid rate that it can sometimes seem whimsical. For example, there are the AdS/CMT (Condensed Matter Theories), AdS/bCFT (boundary CFT), Kerr/CFT, and dS/CFT (de Sitter) correspondences. I'm sure I've left some ones out. There is even a connection (though not quite at the level of a duality) between [quantum error correction](https://en.wikipedia.org/wiki/Quantum_error_correction) and AdS/CFT, which has been called AdS/QEC [^2].

So, continuing in this tradition, it is natural (and probably inevitable) to consider a connection between neural networks and quantum gravity, and let’s just call this still-under-development-connection the AdS/ANN correspondence for fun. In my quest to understand neural networks from a physicist's point of view I haven't been able to think of a concrete connection to AdS/CFT or gravity, and so my title is a bit of a joke, but as we will see, there *are* some interesting connections with other areas of physics, in particular spin-glasses.

## A Brief Introduction of Neural Networks

There are far better sources than me for the history of artificial neural networks, a good starting place is simply Wikipedia. The brief historical overview is that neural networks were originally intended to model actual biological neural networks, and eventually found a second-home in the fields of machine learning and artificial intelligence applications. About 25 years after the first neural networks were proposed, research stagnated, partially due to some damning observations made by Marvin Minsky and Seymour Papert, including the fact that at that time neural networks could not learn the XOR function (XOR stands for exclusive or, so it evaluates to true if exactly one of the boolean inputs is true), as well as the fact that the current computational power was too limited for neural networks to be practical. In 1986, the year I was born, the back-propagation algorithm was published (more on this below) which significantly cut down on the computer power needed to implement neural networks. Since then there has been a resurgence of interest in neural networks, and many of major tech companies are either using neural networks or researching them for commercial implementation. For example, [here](http://www.forbes.com/sites/amitchowdhry/2014/03/18/facebooks-deepface-software-can-match-faces-with-97-25-accuracy/#31de0bd127e5) is an article on Facebook's DeepFace project which is based on neural networks.

At a basic level, a neural network is a network of "neurons" which are themselves inspired by actual biological neurons. Neurons can receive and transmit electrical impulses, and generally speaking, if the signal that the neuron receives is smaller than some threshold, the neuron does nothing. After that threshold is crossed, the neuron fires it's transmitters in strength proportional to the input; so if the input is doubled, the output is also doubled, and this behaviour continues until the neuron saturates it's maximum firing potential. The general response of the neuron is thus generally taken to be an S-shaped (or sigmoid) function. In this blog I'll model this general S-shaped behaviour via the logistic function

$$ \sigma(x) = \frac{1}{1+ \exp(-x) } \, $$

because it is simple and shows up the elementary machine learning technique of logistic regression. There are plenty of other acceptable choices, such as the \\( tanh(x) \\) function. The logistic function does not actually yield the best performance but it is very simple.

<p align="center">
    <img src="/assets/AdSANN/sigmoid.png"/>
</p>

There are many types of neural networks, and they can be used in a wide variety of applications. In this article I want to focus on the a relatively simple kind of network that is nonetheless complex enough to overcome the criticisms of Minsky and Papert.

## The Harmonic Oscillator of Neural Networks

Often the best way to learn something is to get your hands dirty by working out an example. I can't speak for other fields, but in physics there is a tendency to focus on really understanding a simple example, without any of the bells or whistles that may come later, and using this example to build intuition. Perhaps the most famous illustration of this is the harmonic oscillator which corresponds to a particle in a quadratic potential well. This is one of the first and certainly most important of all the systems encountered in both classical and quantum mechanics, and serves as a good foundation to study more complicated systems.

As best I can tell, the "harmonic oscillator" for neural networks is a feed-forward neural network for image recognition applied to the MNIST (Mixed National Institute of Standards and Technology) data set. [Here](https://github.com/gshartnett/datasci/blob/master/NN.ipynb) is a link to a Jupyter notebook I wrote to solve this problem in R. For those physicists reading along, the Jupyter notebook is a really cool programming environment similar to the Mathematica notebook, except that it runs in your browser and can run many different programming languages, including python and R.

The MNIST data set, along with benchmarks of various publications can be found [here](http://yann.lecun.com/exdb/mnist/). The statement of the problem is: given a 28x28 pixel gray-scale image of a handwritten number 0-9, where each pixel is a number from 0 to 255 indicating the intensity, correctly identify the number as often as possible. Here is a typical image:

<p align="center">
    <img src="/assets/AdSANN/MNISTexample.png"/>
</p>

The first step in solving a problem like this is familiar from physics; the problem needs to be translated into mathematical language in some way. In this case, the key is to represent the image as 28x28 matrix (or later as a 784-element flattened vector):

$$ \tiny{\left(
\begin{array}{cccccccccccccccccccccccccccc}
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &
0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &
0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &
0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &
0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 47 & 160 & 195 & 205 & 71 & 86 & 13 & 0 & 0
& 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 12 & 238 & 254 & 254 & 254 & 254 & 254 & 227 &
84 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 13 & 196 & 176 & 127 & 176 & 207 & 236 & 245 &
222 & 23 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 20 & 137 & 254 & 95 & 0
& 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 78 & 254 & 144 & 0 &
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 17 & 200 & 254 & 170 & 0
& 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 47 & 218 & 254 & 216 & 29 &
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 95 & 225 & 254 & 245 & 65 & 0 &
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 12 & 128 & 252 & 254 & 236 & 40 & 0 & 0
& 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 172 & 254 & 254 & 170 & 58 & 0 & 0 & 0 &
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 207 & 254 & 254 & 65 & 0 & 0 & 0 & 0 & 0
& 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 137 & 254 & 254 & 251 & 146 & 26 & 0 & 0
& 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 9 & 173 & 224 & 254 & 254 & 230 & 71 & 0
& 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 8 & 12 & 254 & 254 & 254 & 246 & 104
& 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 46 & 189 & 254 & 254 & 224 &
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 10 & 0 & 0 & 0 & 0 & 0 & 17 & 175 & 254 & 254 &
25 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 35 & 231 & 156 & 112 & 87 & 186 & 191 & 218 & 254 &
254 & 219 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 120 & 255 & 254 & 254 & 254 & 254 & 255 & 254 & 254
& 242 & 67 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 139 & 255 & 254 & 254 & 254 & 254 & 255 & 171 & 46 &
24 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 5 & 71 & 164 & 189 & 174 & 174 & 66 & 4 & 0 & 0 & 0
& 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &
0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &
0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &
0 & 0 & 0 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 &
0 & 0 & 0 & 0 & 0 & 0 & 0 \\
\end{array}
\right)} $$

In order to help us with the task of image recognition, we are given a large number of such images that have been correctly labelled, and the challenge is to use these examples to "learn" a function that does a good job of inferring the correct label, given the image data. So we want a function \\( f : \\{ 0, 1, ..., 255 \\}^{784} \rightarrow  \\{ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 \\} \\) that correctly maps the images to their digit assignment. This is an example of supervised learning, in which the structure of the data is already known. For someone who is unfamiliar with machine learning, this set-up is in many ways similar to linear and logistic regression, which might useful to briefly review.

## Aside: Linear and Logistic Regression

In the simplest manifestation of linear regression, we have a bunch of data points \\( \left( (x_1, y_1), (x_2, y_2), ... (x_N, y_N) \right) \\) and the goal is to find the choice of parameters \\( \theta_0, \theta_1 \\) such that the line \\( y(x) = \theta_0 + \theta_1 x \\) is the best fit to the data. What does 'best fit' mean here? For linear regression, best fit means minimizing the mean squared error,

$$ J(\theta_0, \theta_1) = \frac{1}{N} \sum_{i=1}^N \left(y_i - y(x_i) \right)^2 =\frac{1}{N} \sum_{i=1}^N \left(y_i - \theta_0 - \theta_1 x_i \right)^2  \, .$$

The solution to this problem is simple--just find the \\( \theta \\)'s for which the derivatives with respect to the \\( \theta \\)'s vanishes, and this will be the minimum. J can be thought of as a Hamiltonian for a point particle in a potential well, and the goal is to find the minimal energy configuration. The result might look something like this

<p align="center">
    <img src="/assets/AdSANN/linearregression.png"/>
</p>

This doesn't seem to be very closely related to the problem of image classification, where the \\( y \\) were not real numbers but instead elements of a discrete set. The classification analog of linear regression is known as [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression). The set-up for logistic regression in the simplest case of a binary output \\( y \in \\{ 0, 1 \\} \\) is as follows. What is fitted is not
\\( y \\) directly, but rather the conditional probability \\( \mathbb{P} \left(y^i = 1 \big| x^i \right) \\), the probability that the data point corresponds to 1 given the data point \\( x^i \\). If this quantity is \\( \gt 1/2 \\) then the most likely output is 1, otherwise it is 0. The way this works in formulae is that

$$ p^i \equiv\mathbb{P} \left(y^i = 1 \big| x^i \right) = \sigma (\theta_0 + \theta_1 x^i) \, , $$

where \\( \sigma \\) is the sigmoid function introduced above. The parameters \\( \theta \\) are again fitted by a minimization principle, this time with the cross-entropy cost function

$$ J(\theta) = \, -\frac{1}{N} \sum_{i=1}^N \left( \delta_{1, y^i} \log p^i + \delta_{0, y^i} \log (1-p^i) \right) \, ,$$

One of the nice things about choosing the logistic function to parametrize the probabilities here (and this is indeed a choice) is that it leads to a linear estimation for the log-odds, i.e.

$$ \log \left( \frac{p^i}{1-p^i} \right) = \theta_0 + \theta_1 x^i \, . $$

You might be wondering why one would choose the above cost function, and I could go on to justify this, but why bother when Terry Tao has an [excellent blog post](https://terrytao.wordpress.com/2016/06/01/how-to-assign-partial-credit-on-an-exam-of-true-false-questions/) where he essentially re-discovered it from scratch. The TL;DR is that minimizing this function results in a estimate for the probabilities that is most likely to align with the true probability distribution. I should, however, point out that one nice thing about this choice of cost function and the sigmoid function is that the cost function is convex which among other things means that any local minimum is guaranteed to be the global minimum.

## Back to the MNIST "Harmonic Oscillator"

With that lightning review/introduction of logistic regression out of the way, I can now get to the fun stuff. So from the above, we can surmise that there exist old and well-established methods for classification problems. What can the neural network offer us beyond logistic regression? The key-feature of logistic regression is that the probabilities are assumed to take a special parametric dependence on the data \\( x \\). In particular, the log-odds are linear in \\( x \\).

A very unenthusiastic but accurate way of describing neural networks is that they simply parametrize a certain class of non-linear functions. The reason why neural networks are so interesting is because the class of functions that they describe possess a hierarchical structure which has proven very useful for tasks like image recognition. This hierarchical structure is part of why these networks are attractive to a physicist such as myself, and in fact there is much excitement in various physics communities over the so-called [tensor-networks](https://www.quantamagazine.org/20150428-how-quantum-pairs-stitch-space-time/), which use a different type of network to hierarchically encode entanglement in quantum systems.

So enough talk, let me actually present an example of a neural network. In the MNIST case the neural network takes as an input a 784-dimensional vector associated with an image of a digit. I represented this above as a matrix but now I'll flatten it into a vector. The output of the neural network is a 10-dimensional vector with the \\(k\\)-th component a measure of the likelihood that the input corresponds to the digit \\(k - 1 \\). So for example, the output corresponding to the i-th image vector input might look like this:

$$ p^i = \{ 0.50, 0.46, 0.62, 0.67, 0.31, 0.57, 0.37, 0.30, 0.33, 0.54 \} $$

Unlike logistic regression, the entries \\(p_k^i\\) (with \\(k = 1...10 \\) ) are no longer probabilities, and in particular are not constrained to sum to 1, but there are ways to use these to estimate the probabilities.[^3] The largest entry is taken to indicate the most likely class assignment. So in this case the number would most likely be given the assignment of 3.

The way the neural network takes a 784-dimensional input and returns this 10-dimensional output is a just a series of matrix multiplications followed by the application of the sigmoid activation function. This process is most conveniently described pictorially:

<p align="center">
    <img src="/assets/AdSANN/NNdrawer-pics-pdf-291x300.jpg"/>
</p>

In equations, this reads

$$ \begin{align}
& \text{(input layer):} \qquad a^{(1)}_{ij} = x^{(i)}_j, \\
& \text{(hidden layer):} \qquad z^{(2)}_{ij} = \sum_m^{M+1} \Theta^{(1)}_{im} \tilde{a}^{(1)}_{mj}, \qquad a^{(2)}_{ij} = \sigma( z^{(2)}_{ij}) \, , \\
& \text{(output layer):} \qquad z^{(3)}_{ij} = \sum_p^{P+1} \Theta^{(2)}_{ip} \tilde{a}^{(2)}_{pj}, \qquad p^i_k = a^{(3)}_{ki} = \sigma( z^{(3)}_{ki}) \, .
\end{align} $$

Here \\(M \\) is the dimension of the input, 784 in this case, \\(P \\) is the number of neurons in the hidden layer, and \\( K \\) is the number of distinct classes, 10 in this case. The way to translate between the diagram and the equation is as follows. The first layer of nodes represents the input data, \\( x^i \\), with a node for each entry in the M-element vector in the diagram. The lines connecting the first layer of nodes to the second are parameters of the network known as weights. These are conveniently lumped into a matrix \\( \Theta^{(1)} \\). After matrix multiplication by the weights, the sigmoid activation function \\( \sigma \\) is applied to each output, introducing a certain type of non-linearity. This intermediate output then undergoes another round of matrix multiplication (this time by \\( \Theta^{(2)} \\)), followed by yet another application of \\( \sigma \\). Finally, we arrive at the output \\( p_k \\).

You might be wondering what the tilde's stand for, what the "\\( + 1 \\)"'s stand for in the diagram, or why the sum limits are increased by 1. This is because at each layer (except the last layer) I have introduced what is known as a bias unit for the same reason that the constant term is included in linear and logistic regression.

This is known as a feed-forward ANN. I chose to work with 1 hidden layer here for simplicity, but the number of such layers is completely arbitrary, as is the number of units in each hidden layer. At the end of the day, this network just describes a certain parametric family of non-linear functions, albeit one that has proven very useful for many machine learning applications.

Having described the class of functions parametrized by neural networks, I'll turn now to the hard part, which is finding a choice of the weights so that the network does a good job of classifying images. The cost function I'll use is the straightforward generalization of the cross-entropy for the binary classification problem

$$ J(\theta) = \, -\frac{1}{N} \frac{1}{K} \sum_{i=1}^N \sum_{k=1}^K \left( \delta_{k, y^i} \log p^i_k + (1-\delta_{k, y^i}) \log (1-p^i_k) \right) \, ,$$

The main difficulty here is that the cost function \\( J \\) is no longer convex. This means that there are now saddles and local minima that can complicate the optimization procedure. There are plenty of optimization algorithms. For my Jupyter notebook, I chose to use perhaps the simplest and most common algorithm, gradient descent. In gradient descent a sufficiently small learning rate \\( \alpha \\) is chosen, and the parameters of the function update according to the rule

$$ \Theta^{(\ell)} \rightarrow \Theta^{(\ell)} - \alpha \nabla_{\Theta^{(\ell)}} J \, .$$

A small step is taken in the direction where the function is decreasing at the highest rate. This isn't a very sophisticated optimization algorithm, there are others such as Newton-Raphson, but the key here is that for typical problems there can be huge matrices involved, and so it might be prudent to use a method that takes many steps to converge if each step can be computed very quickly relative to another optimization strategy. Also, the calculation of the gradients is a non-trivial task; if done in a naive way the algorithm will be prohibitively slow due to the many necessary matrix multiplications. The famous Back-Propagation algorithm I mentioned earlier alleviates this problem through a clever use of the chain rule.

I implemented a neural network like the kind discussed here There are 60,000 labelled images that may be used for the training step, and another 10,000 labelled ones in a test data set that can be used to benchmark the algorithm. With 2 hidden layers of 300 and 100 units each, I was able to get the error rate on the test set down to about 6%. Comparing with [public benchmarks](http://yann.lecun.com/exdb/mnist/), this is pretty good, but it's not as good as it could be. It's certainly not cutting edge. Moreover, it seems some conceptually simpler methods should be able to do as good or better with less effort.

One thing that I became really hung-up on throughout this process was the convergence of the gradient descent training procedure. The code *never seemed to converge*. Perhaps I just wasn't running it for long enough. Here's a typical plot of the training cost \\( J \\) throughout the training

<p align="center">
    <img src="/assets/AdSANN/trainingcost.png"/>
</p>

The curve is not monotonically decreasing because in this particular run I took a fairly aggressive training strategy where the learning rate \\( \alpha \\) is initially quite high. There is a bit of an art to choosing the right learning rate \\( \alpha \\). Too high and you'll risk overshooting minima, and too low and the training will take forever. So here I decided to set alpha to be recklessly high, and then to decrease it every time the cost function increased, indicating that I overshot. I also periodically decayed the learning rate, which from what I've read is good general practice. From the plot it seems clear that if I let the code run longer the cost would barely decrease at all.

At first I was worried that this was a bug and tried all sorts of learning strategies, and let the code run for really long periods of time. I even checked that I was calculating the gradients correctly via back-propagation by comparing them to the gradients obtained by simple finite differencing, i.e.

$$ \frac{d J}{d\theta} = \frac{J(\theta + h) - J(\theta - h)}{2h} + O(h^2) \, , $$

and the two methods agreed (this method of computing derivatives is much slower than back-propagation which is why it's only used for checks like this). I no longer think this is a bug in my code and in fact I think that there is a really interesting explanation for this behavior which has led me to make some connections with physics.

## The False Boogeyman of Local Minima

In reading and watching online talks about neural networks, you'll often hear about the problem of local minima. Because the cost function \\( J \\) is not convex, there is no reason why a minima must be the global minima, and there is a danger that the optimization algorithm could find itself stuck in a local minima. If this occurred, and the local minima was pretty close in cost to the global minima, then this wouldn't be much of a problem. The real worrying situation is when the local minima has a relatively high cost. Fun aside that I can't help but mention: Local minima play a really interesting role in Quantum Field Theory where they are known as false vacua. False vacua lead to the cool-sounding phenomenon of False Vacuum Decay, about which perhaps my favorite paragraph ever to appear in a physics paper was written[^4]:

*This is disheartening. The possibility that we are living in a false vacuum has never been a cheering one to contemplate. Vacuum decay is the ultimate ecological catastrophe; in a new vacuum there are new constants of nature; after vacuum decay, not only is life as we know it impossible, so is chemistry as we know it. However, one could always draw stoic comfort from the possibility that perhaps in the course of time the new vacuum would sustain, if not life as we know it, at least some structures capable of knowing joy. This possibility has now been eliminated.*

<p align="center">
    <img src="/assets/AdSANN/falsevacuum.png"/>
</p>

Fortunately, the problem of local minima in the context of neural network machine learning isn't quite so dire! At some point when my code wasn't converging, I threw up my hands and said "well, it must be the local minima". But if a local minimum was really reached, then the code should converge! I began to dig around, and I think I found the source of the problem in this lovely paper[^5] which in turn built upon the results of this paper on random matrix theory of Bray and Dean[^6]. I highly recommend these papers to anyone from a physics background interested in machine learning.

Consider a random field \\( \phi(x_1, ... , x_N) \\)  (with some as-yet unspecified distribution) in an \\( N \\)-dimensional space, and consider further the collection of points where the gradient vanishes (i.e. critical points). A natural question is: what fraction of these critical points are saddles, as opposed to local minima/maxima? The critical points are characterized by the eigenvalues of the Hessian

$$ H_{ij} = \partial_i \partial_j \phi \, .$$

If all the eigenvalues are positive (negative) then the critical point is a global minimum (maximum). If eigenvalues of both sign appear (and there are no zero eigenvalues) then it is a saddle. A heuristic way to think about a randomly distributed function at a critical point is that the eigenvalues of \\( H_{ij} \\) will themselves be random variables with no bias towards positive or negative values. Proceeding with this logic then, a given eigenvalue has a 50% chance of being positive and a 50% chance of being negative (the probability of having an exactly zero eigenvalue should be infinitesimally small), and so the probability that all \\( N \\) eigenvalues be negative, corresponding to a local minima, is \\( 2^{-N} \\), exponentially small in \\( N \\) (perhaps this should be taken with a grain of salt since a bounded function needs to have at least 1 minima). This is essentially the result found by Bray and Dean for the case of gaussian-distributed random fields, and of course they did more work than the argument I made just now--they also computed the distribution itself.  They were also able to calculate that when local minima did occur, they were exponentially likely to occur close in value to the global minima; in other words, "dangerous" local minima with high energies/costs are very rare.

"This is all very nice, but what does this have to do with neural networks?", you ask. Well, the neural network problem is naturally a high-dimensional problem. The cost function \\( J \\) is a function of the \\(\Theta^{(\ell)} \\) matrices, and for the neural network I trained for this problem, with 2 hidden layers of 300 and 100 units each, the number of parameters in these matrices is 266,610. Now, to be fair, the cost function of a neural network is almost certainly not a gaussian random field, but I see no reason that the statistics of its critical points should behave qualitatively differently than above. I would expect that the assumption of a gaussian-distributed random field is mainly meant to make the problem tractable. This was also Dauphin et al's expectation, and they verified empirically that the same phenomena observed in the Bray & Dean paper is exhibited by neural networks.

<p align="center">
    <img src="/assets/AdSANN/coinflip.gif"/>
</p>

Armed with this result, let me return to my convergence issue. So the problem can't be that I got stuck in a local minima, because the gradient descent didn't converge, and in light of the above results, most critical points are in fact saddles anyhow. So the working-intuition that I've developed is that the landscape of the cost function \\( J \\) is apparently characterized by what you might call very shallow riverbeds, where there are steep uphill gradients in some directions, but the downhill gradients are very shallow. So it could just take a very very long time for gradient descent to actually converge. I admit that I haven't thought much about the landscapes of functions in high dimensional spaces, and certainly not about the intricacies of neural network machine learning, and I'm sure that there are important results I'm unaware of.

## Automating HEP-TH

A fun tangent I spent some time exploring was the possibility of using a neural network to automate my job and produce high impact hep-th (high energy physics - theory) papers. Perhaps the publish-or-perish mentality was weighing too heavily on me as a newly minted postdoc, or perhaps I just wanted the same sweet gig that [this software engineer set-up](http://interestingengineering.com/programmer-automates-job-6-years-boss-fires-finds/). I took this idea from [this very nice blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy in which he used a more sophisticated type of neural network than the one shown above to generate artificial Shakespeare plays, Wikipedia entries, algebraic geometry proofs, and even OS source code. So I decided to follow suite and see if I could get a neural network to generate hep-th abstracts, using all the abstracts from 1992-2003 as training data. This data is available [here](http://www.cs.cornell.edu/projects/kddcup/datasets.html), as well as all the TeX source code for the papers during this same period. I collected these abstracts into one giant 32.5Mb .txt file. Here is an example excerpt:

```
------------------------------------------------------------------------------
\\
Paper: hep-th/9711200
From: Juan Maldacena <malda@physics.rutgers.edu>
Date: Thu, 27 Nov 1997 23:53:13 GMT (22kb)
Date (revised v2): Mon, 8 Dec 1997 18:59:11 GMT (23kb)
Date (revised v3): Thu, 22 Jan 1998 15:42:41 GMT (23kb)

Title: The Large N Limit of Superconformal Field Theories and Supergravity
Authors: Juan M. Maldacena
Comments: 20 pages, harvmac, v2: section on AdS_2 corrected, references added,
 v3: More references and a sign in eqns 2.8 and 2.9 corrected
Report-no: HUTP-98/A097
Journal-ref: Adv.Theor.Math.Phys. 2 (1998) 231-252; Int.J.Theor.Phys. 38 (1999)
 1113-1133
\\
 We show that the large $N$ limit of certain conformal field theories in
various dimensions include in their Hilbert space a sector describing
supergravity on the product of Anti-deSitter spacetimes, spheres and other
compact manifolds. This is shown by taking some branes in the full M/string
theory and then taking a low energy limit where the field theory on the brane
decouples from the bulk. We observe that, in this limit, we can still trust the
near horizon geometry for large $N$. The enhanced supersymmetries of the near
horizon geometry correspond to the extra supersymmetry generators present in
the superconformal group (as opposed to just the super-Poincare group). The 't
Hooft limit of 4-d ${\cal N} =4$ super-Yang-Mills at the conformal point is
shown to contain strings: they are IIB strings. We conjecture that
compactifications of M/string theory on various Anti-deSitter spacetimes are
dual to various conformal field theories. This leads to a new proposal for a
definition of M-theory which could be extended to include five non-compact
dimensions.
\\
------------------------------------------------------------------------------
\\
Paper: hep-th/9711201
From: BIANCHI@axtov1.roma2.infn.it
Date: Fri, 28 Nov 1997 00:54:53 GMT (18kb)
Date (revised v2): Fri, 7 Aug 1998 10:00:56 GMT (23kb)

Title: A Note on Toroidal Compactifications of the Type I Superstring and Other
 Superstring Vacuum Configurations with 16 Supercharges
Author: Massimo Bianchi, (Dipartimento di Fisica, Universita` di Roma ``Tor
 Vergata'')
Comments: Expanded discussion of rational un-orientifolds. Comments on
 disconnected components of type I moduli spaces. Several references added.
 Final version to appear in Nuclear Physics B528 (1998). 22 pages, Latex
Report-no: ROM2F-97/53
Journal-ref: Nucl.Phys. B528 (1998) 73-94
\\
 We show that various disconnected components of the moduli space of
superstring vacua with 16 supercharges admit a rationale in terms of BPS
un-orientifolds, i.e. type I toroidal compactifications with constant
non-vanishing but quantized vacuum expectation values of the NS-NS
antisymmetric tensor. These include various heterotic vacua with reduced rank,
known as CHL strings, and their dual type II (2,2) superstrings below D=6. Type
I vacua without open strings allow for an interpretation of several
disconnected components with N_V=10-D. An adiabatic argument relates these
unconventional type I superstrings to type II (4,0) superstrings without
D-branes. The latter are connected by U-duality below D=6 to type II (2,2)
superstrings. We also comment on the relation between some of these vacua and
compactifications of the putative M-theory on unorientable manifolds as well as
F-theory vacua.
\\
------------------------------------------------------------------------------
```

Producing artificial text of this form seems like a fairly complex task. In addition to basic grammar, the network needs to learn a vocabulary of technical phrases such as "Seiberg-Witten" or "Topological quantum field theory". It also needs to learn the "meta-structure" of the abstracts, for example entries are separated by "-----------------------", there are fields such as "Paper, From, Date, Title", etc. There's even TeX code interspersed in the abstracts that needs to be learned!

I trained the same kind of neural network Andrej used, using code he made available (actually, there is [this](https://github.com/jcjohnson/torch-rnn) updated version provided by his colleague Justin Johnson which is much faster). I ran the code for about 5 days on my Lenovo W530 ThinkPad laptop (weird aside: after 5 days the laptop crashed and had "forgotten the internet" so I needed to re-install some critical libraries in order to reconnect), and here's a random sample of the output:

```
-------------------------------------------------------------------------------
\\
Paper: hep-th/0003121
From: Bogouri Ishikawa <makajimato@wicc.weizmann.ac.il>
Date: Thu, 22 Jan 97 02:26:08 MES (2kb)

Title: Basis of the Correlation function of a $sk_2$ boson field
Authors: A.N.Kempf. & J. Scharf
Comments: Papers and corrections, references added
Report-no: IASSNS-HEP--00/9
Journal-ref: Fortsch.Phys. 47 (1999) 103-116
\\
 We investigate after a generalization of the starting point for noncommutative
$\epsilon$ that naturally appears to be a Poincar\'{e}
fractional gerbe. Asymptotically flat v^6 Misarchies do allow us
to solve some space-time quantities in this process. We derivate the
difference between these total effective actions to be produced on Kaehler
string. In terms of the spin Calogero-Sutherland's coefficients are also
discussed.
\\
-----------------------------------------------------------------------------
\\
Paper: hep-th/9809246
From: Oleg Lunin <luni@if.ufrgs.br>
Date: Wed, 8 Dec 2000 06:19:25 GMT (21kb)
Date (revised v2): Tue, 11 Oct 2000 11:39:04 GMT (23kb)

Title: Mhminnimal Axion Case in
 M-Theory
Authors: Noureddine Mohammerd, T.J. Hawhi (KEK)
Comments: 13 pages, harvmac. Contribution to the JHEP loop Conference
 Physics, Fourtra 1999, Boston - STRINGS924
Report-no: MIT-CTP #463
Subj-class: High Energy Physics - Theory; Mathematical Physics; Perturbation
 Theory
Journal-ref: Phys.Rev. D62 (2000) 085016
\\
 We discuss exact solutions of k=2 supersymmetric theories with a single
maximal diagonal size. We also discuss, at one-loop
points this leads to the lattice, and consider the role of additional graviton
wavefunctions of a group $SU(2,1)$. It is argued that vertex operators with
$[0$ has a (bar index $l_{r,\bar{{\bf F}}}$) into negative dimensional {\it
discrete}\21$ contact. The double- importance are made related to causality-interest in
terms of the couplings energies and momentum and rotationally invariant
version.
\\
-----------------------------------------------------------------------
```

So I'm not quite there yet--a quick glance tells you that this is nonsense, so I'll have to work harder if I really want to push up my h-index. But it is impressive how much it has learned. The network "knows" the overall structure, plenty of proper nouns like Axion or Poincare, it even knows that Oleg Lunin (who happens to be a real physicist) is a reasonable name. There are some funny inconsistencies, for example the second paper was *submitted* by Oleg Lunin but it was *authored by* Noureddine Mohammerd, and T.J. Hawhi. Normally one of the authors is the submitter. Another one is that the revision date is 2 months earlier than the initial submission date!

```
Date: Wed, 8 Dec 2000 06:19:25 GMT (21kb)
Date (revised v2): Tue, 11 Oct 2000 11:39:04 GMT (23kb)
```

I think I could have gotten a better result if I was willing to train a larger network for a longer period of time. I didn't want to let the thing run for a month straight, though. Not long after I finished playing with this, I read [this](http://arstechnica.com/the-multiverse/2016/06/an-ai-wrote-this-movie-and-its-strangely-moving/) fun article where essentially the same idea was applied to Sci-Fi movie scripts, and in this case they hired a cast of fairly well-known actors to act out a few scenes, resulting in a somewhat amusing short film.

## Some Connections with Physics

In this last section I'll pivot and discuss some loose connections with physics in order to establish my claim to the AdS/ANN correspondence (like pre-emptively buying a soon-to-be-popular domain name). At first I didn't think there was much hope for interesting connections between neural networks and physics. As I mentioned above, one way of looking at neural networks is simply as a certain class of parametric functions. In this view it would be hard to imagine any interesting connections. For example, logistic regression is a parametric class of models of the form

$$ y = \sigma\left( \theta_0 + \theta_1 x \right) $$

if the sigmoid function ever showed up in a physics context, you wouldn't argue that there is a deep connection between logistic regression and physics, just that the same class of function shows up in both.

That being said, I have come to learn that there are some connections. First of all, neural networks have been successfully used in at least one observational/experimental context, which is classifying galaxies. I think this is actually a very exciting connection that is going to continue to grow in all science disciplines as more data and computing power becomes available. In this post I'm more interested with theoretical connections.

From what I can gather, the most well-established connection is with condensed matter systems known as a spin-glasses [^7] [^8]. Spin-glasses are magnetic systems in which the magnetic spins are not aligned, but instead possess a disorder reminiscent of glasses. The key feature is that the spins are frustrated, or constrained, so that there is an interesting competition of effects leading to this unconventional phase of matter. After learning about the above result of Bray and Dean, the connection between spin-glasses and neural networks intrigues and confuses me a bit because I have heard often that spin-glasses have lots of metastable minima (this is another physics term for local minima) and very slowly relax to the configuration of lowest energy as a result of thermal fluctuations. If this is true, then they somehow avoid the Bray-Dean result that almost all critical points are saddles. One obvious possibility is that the assumption of gaussianity is far more restrictive than my intuition led me to believe. Perhaps it's worth pointing out that the Bray-Dean result is relatively recent, and so there very well might be more to this story than has been explored thus far, and I think this might be an interesting road to go down. Maybe as a last point it is worth mentioning that for certain spin-glasses, the problem of relaxation to the ground state has been shown to be NP-hard [^9], and so it would be very interesting to understand just how these two systems relate to one another.

This same Bray-Dean result has been applied to few topic that is closer to home, which involves the bipartite entanglement entropy of pure states [^10], where they calculate the probability distributions for the Renyi entropies of a random pure state. This is relevant for quantum information, a field which is increasingly being connected with quantum gravity. Speaking of quantum information, neural networks also have lots of interesting connections with computer science. Hornik et al[^11] has shown that the type of neural network I've discussed here are universal approximators in the sense that they can simulate any function to any accuracy (given a large enough network). There are also such things as quantum neural networks and quantum machine learning.

So perhaps it's time to wrap this up. Unfortunately, I do not see any hope (yet) for a true AdS/ANN correspondence, but who knows. Neural networks can be related to spin-glasses, and there have been some attempts to model spin-glasses holographically, so it's not crazy that to think that a connection between neural networks and AdS might be possible, although I'm not sure how insightful this would be. In any case, I can say that in the short time that neural networks have had my attention, I have uncovered some very interesting parallels with other fields. I am certain that there are plenty of connections that I have missed that others have already made, and probably some more yet to be discovered.

[comment]: <> (footnotes)
[^1]: Polchinski J., [http://arxiv.org/abs/1412.5704](http://arxiv.org/abs/1412.5704)
[^2]: Almheiri A. et al [http://arxiv.org/abs/1411.7041](http://arxiv.org/abs/1411.7041)
[^3]: Denker, J. and LeCun, Y. [Transforming Neural-Net Output Levels to Probability Distributions](http://yann.lecun.com/exdb/publis/pdf/denker-lecun-91.pdf)
[^4]: Coleman, S. and De Luccia, F. [http://journals.aps.org/prd/abstract/10.1103/PhysRevD.21.3305](http://journals.aps.org/prd/abstract/10.1103/PhysRevD.21.3305)
[^5]: Dauphin Y. et al. [http://papers.nips.cc/paper/5486-sparse-pca-via-covariance-thresholding](http://papers.nips.cc/paper/5486-sparse-pca-via-covariance-thresholding)
[^6]: Bray A. and Dean D. [https://arxiv.org/abs/cond-mat/0611023](https://arxiv.org/abs/cond-mat/0611023)
[^7]: Kappen, B. [Computational physics: Neural Networks](http://www.snn.ru.nl/~bertk/comp_phys/handouts.pdf)
[^8]: Sompolinksy, H. [Statistical Mechanics of Neural Networks](https://nms.kcl.ac.uk/reimer.kuehn/CS03/Sompolinsky_PhysicsToday.pdf)
[^9]: Baharona, F. [On the computational complexity of Ising spin models glass](http://iopscience.iop.org/article/10.1088/0305-4470/15/10/028/meta;jsessionid=EF0229949F1B87C969852D71784EC280.c3.iopscience.cld.iop.org)
[^10]: Nadal, C. et al [Phase Transitions in the Distribution of Bipartite Entanglement of a Random Pure State](http://arxiv.org/abs/0911.2844)
[^11]: Hornik, K. et al [Multilayer feedforward networks are universal approximators](http://www.sciencedirect.com/science/article/pii/0893608089900208)

{% include disqus.html %}
